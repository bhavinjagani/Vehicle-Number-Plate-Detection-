## Vehicle Number Plate Detection (YOLOv9 & YOLOv5 + EasyOCR)

This repository implements a dual-model automatic license plate recognition (ALPR) pipeline that trains both YOLOv9 and YOLOv5 detectors on the **same curated dataset** and feeds their detections into **EasyOCR** for plate transcription. The goal is to compare performance across architectures while maintaining a single source of truth for data, evaluation, and reporting.

---

### 1. Motivation & Problem Statement
Manual plate transcription is inaccurate and slow for tolling, parking automation, and traffic analytics. Classical CV pipelines collapse under varied lighting, motion blur, and diverse plate layouts. By combining modern real-time detectors (YOLOv9 for accuracy, YOLOv5 for maturity/speed) with EasyOCR, we can deliver a production-ready, fully open-source solution that remains robust across:

- Multiple countries (OpenALPR benchmark mix, AOLP, Indian plates).
- Environmental conditions (night, rain, shadow, partial occlusion).
- Input modalities (videos, webcam feeds, and still images).

---

### 2. Repository Structure
```
.
├── configs/
│   └── datasets.yaml          # Dataset unification + augmentation switches
├── data/
│   └── processed/             # Generated YOLO-format dataset (created later)
├── external/                  # Cloned YOLOv5 / YOLOv9 repos (bootstrap script)
├── scripts/
│   └── bootstrap_models.sh    # Clones upstream detectors at pinned commits
├── src/
│   ├── data/prepare_dataset.py   # Merges OpenALPR, Indian, AOLP into one corpus
│   ├── inference/detect_and_read.py # YOLO detections + EasyOCR transcription
│   └── train.py                  # Unified trainer wrapper for YOLOv5 & YOLOv9
├── requirements.txt
└── README.md
```

---

### 3. Datasets & Preparation
| Dataset | Coverage | Notes |
|---------|----------|-------|
| OpenALPR Benchmark | Multi-country, varied lighting | Ideal for generalization |
| Indian License Plate (Sanyam Goyal) | Region-specific fonts/formats | Balances script diversity |
| AOLP (Academia Sinica) | Real-world traffic cams | Introduces occlusion + low light |

1. **Download** each dataset manually (licensing restrictions prevent redistribution).
2. **Annotate/convert** to YOLO format (txt label per image) if not already provided.
3. Update `configs/datasets.yaml` with the absolute paths to your local copies.
4. Run the merger:
   ```
   python src/data/prepare_dataset.py --config configs/datasets.yaml
   ```
   The script copies/symlinks every split into `data/processed/{train,val,test}/(images|labels)` and emits a manifest for reproducibility.

---

### 4. Environment & Dependencies
```
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

> **GPU:** CUDA 12.x + torch 2.2+ recommended. EasyOCR will automatically switch to CPU if no CUDA device is visible.

---

### 5. Bootstrapping YOLO Repos (one-time)
```
bash scripts/bootstrap_models.sh
```
The script:
- clones `ultralytics/yolov5` and `WongKinYiu/yolov9` into `external/`
- checks out stable commits
- installs each repo’s native requirements to ensure compatibility

---

### 6. Training on a Shared Dataset
Both models consume the unified `data/processed/data.yaml` (auto-generated by the prep script) guaranteeing identical train/val/test splits.

**YOLOv5:**
```
python src/train.py \
  --model yolov5 \
  --data data/processed/data.yaml \
  --weights yolov5s.pt \
  --img-size 640 \
  --epochs 150 \
  --batch-size 32
```

**YOLOv9:**
```
python src/train.py \
  --model yolov9 \
  --data data/processed/data.yaml \
  --weights yolov9-c.pt \
  --img-size 640 \
  --epochs 150 \
  --batch-size 32
```

Flags such as `--device 0,1`, `--patience`, `--hyp` or any upstream-native arguments can be appended via `--opts ...`.

All experiment artifacts are routed to `runs/{train_yolov5,train_yolov9}/<exp_name>/`.

---

### 7. Detection + EasyOCR Pipeline
After training, point the inference script to either checkpoint:
```
python src/inference/detect_and_read.py \
  --model-type yolov9 \
  --weights runs/train_yolov9/exp/weights/best.pt \
  --source samples/highway.mp4 \
  --ocr-langs en \
  --save-video \
  --output-dir artifacts/runs
```

Key features:
- unified inference path for images, folders, video files, RTSP, or webcam (`--source 0`)
- optional crop dumps + CSV/JSON manifest for downstream analytics
- overlays detection boxes + OCR text directly on frames
- configurable OCR beam search, confidence thresholds, and GPU usage

---

### 8. Evaluation & Reporting
1. **Detector Metrics:** mAP@0.5 and mAP@0.5:0.95 gathered per experiment directory.
2. **OCR Accuracy:** script emits per-plate text + confidences to `artifacts/runs/<exp>/results.jsonl`
3. **Speed:** inference script logs FPS for videos/webcams.
4. **Qualitative Demo:** capture screen or saved video output for presentation deliverable.

Summarize in the performance report:
- data split ratios and augmentations
- hyperparameters for both detectors
- confusion cases (motion blur, glare, overlapping plates)
- side-by-side YOLOv5 vs YOLOv9 qualitative shots

---

### 9. References
1. Ultralytics, “YOLOv9,” https://github.com/WongKinYiu/yolov9  
2. Ultralytics, “YOLOv5,” https://github.com/ultralytics/yolov5  
3. JaidedAI, “EasyOCR,” https://github.com/JaidedAI/EasyOCR  
4. OpenALPR Benchmark, https://github.com/openalpr/benchmarks  
5. Sanyam Goyal, “Indian Licence Plate Dataset,” https://www.kaggle.com/datasets/sanyamgoyal/number-plate-detection  
6. AOLP Dataset, http://aiia.iis.sinica.edu.tw/~ihj/aolp/

---

### 10. Next Steps
- [ ] Finalize dataset downloads + update config paths  
- [ ] Run `prepare_dataset.py` to guarantee consistent YOLO splits  
- [ ] Launch paired training jobs (v5 + v9)  
- [ ] Compare metrics, capture demo footage, and publish documentation
